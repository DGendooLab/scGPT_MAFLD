{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q AnnData scanpy scvi"
      ],
      "metadata": {
        "id": "95yIlQKw8FQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4540eba-8372-4de9-9e9f-11ca498a2573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"datetime.datetime.utcnow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FftscBvhNM_H",
        "outputId": "a25ed2e7-5b80-437d-d1f4-c525e95e47a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.3.0 torchtext==0.18.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JEyxGjjzLUqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scgpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W45ah31ZP9X",
        "outputId": "110680cf-c696-4c92-804a-add7182d9e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.3.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoWGWdF3718l",
        "outputId": "0152fc81-0cc7-4c12-c004-56fc1a6ab983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
            "  warnings.warn(\"flash_attn is not installed\")\n",
            "/usr/local/lib/python3.12/dist-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
            "  warnings.warn(\"flash_attn is not installed\")\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import copy\n",
        "import gc\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "from typing import List, Tuple, Dict, Union, Optional\n",
        "import warnings\n",
        "import pandas as pd\n",
        "# from . import asyn\n",
        "import pickle\n",
        "import torch\n",
        "from anndata import AnnData\n",
        "import scanpy as sc\n",
        "import scvi\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import wandb\n",
        "from scipy.sparse import issparse\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext._torchtext import (\n",
        "    Vocab as VocabPybind,\n",
        ")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "import scgpt as scg\n",
        "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
        "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
        "from scgpt.loss import (\n",
        "    masked_mse_loss,\n",
        "    masked_relative_error,\n",
        "    criterion_neg_log_bernoulli,\n",
        ")\n",
        "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
        "from scgpt.preprocess import Preprocessor\n",
        "from scgpt import SubsetsBatchSampler\n",
        "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
        "\n",
        "sc.set_figure_params(figsize=(6, 6))\n",
        "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Specify hyper-parameter setup for cell-type annotation task"
      ],
      "metadata": {
        "id": "moF-dxCHoyUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_defaults = dict(\n",
        "    seed=0,\n",
        "    dataset_name=\"guilliams_dataset\",\n",
        "    do_train=True,\n",
        "    load_model=\"/content/drive/MyDrive/projects/scGPT-MAFLD/scGPT_human\",\n",
        "    mask_ratio=0.0,\n",
        "    epochs=10,\n",
        "    n_bins=51,\n",
        "    MVC=False, # Masked value prediction for cell embedding\n",
        "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
        "    dab_weight=0.0,\n",
        "    lr=1e-4,\n",
        "    batch_size=32,\n",
        "    layer_size=128,\n",
        "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
        "    dropout=0.2,  # dropout probability\n",
        "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
        "    save_eval_interval=5,\n",
        "    fast_transformer=True,\n",
        "    pre_norm=False,\n",
        "    amp=True,  # Automatic Mixed Precision\n",
        "    include_zero_gene = False,\n",
        "    freeze = False, #freeze\n",
        "    DSBN = False,  # Domain-spec batchnorm\n",
        ")"
      ],
      "metadata": {
        "id": "wFhA77GoZoXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    config=hyperparameter_defaults,\n",
        "    project=\"scGPT_finetune\",\n",
        "    reinit=True,\n",
        "    settings=wandb.Settings(start_method=\"fork\"),\n",
        ")\n",
        "config = wandb.config\n",
        "print(config)\n",
        "\n",
        "set_seed(config.seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "CVUde6Iifj24",
        "outputId": "968305ff-07cd-4cc4-e3c1-8161f3b980ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanatchitnis\u001b[0m (\u001b[33msanatchitnis-university-of-birmingham\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251130_112618-uz8xnlu0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune/runs/uz8xnlu0' target=\"_blank\">ethereal-sound-1</a></strong> to <a href='https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune' target=\"_blank\">https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune/runs/uz8xnlu0' target=\"_blank\">https://wandb.ai/sanatchitnis-university-of-birmingham/scGPT_finetune/runs/uz8xnlu0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seed': 0, 'dataset_name': 'ms', 'do_train': True, 'load_model': '/content/drive/MyDrive/projects/scGPT-MAFLD/scGPT_human', 'mask_ratio': 0, 'epochs': 10, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0, 'dab_weight': 0, 'lr': 0.0001, 'batch_size': 32, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# settings for input and preprocessing\n",
        "pad_token = \"<pad>\"\n",
        "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
        "mask_ratio = config.mask_ratio\n",
        "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
        "\n",
        "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
        "max_seq_len = 3001\n",
        "n_bins = config.n_bins\n",
        "\n",
        "# input/output representation\n",
        "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
        "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
        "\n",
        "# settings for training\n",
        "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
        "CLS = True  # celltype classification objective\n",
        "ADV = False  # Adversarial training for batch correction\n",
        "CCE = False  # Contrastive cell embedding objective\n",
        "MVC = config.MVC  # Masked value prediction for cell embedding\n",
        "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
        "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
        "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
        "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
        "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
        "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
        "adv_D_delay_epochs = 0\n",
        "mvc_decoder_style = \"inner product\"\n",
        "ecs_threshold = config.ecs_thres\n",
        "dab_weight = config.dab_weight\n",
        "\n",
        "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
        "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
        "\n",
        "per_seq_batch_sample = False\n",
        "\n",
        "# settings for optimizer\n",
        "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
        "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
        "batch_size = config.batch_size\n",
        "eval_batch_size = config.batch_size\n",
        "epochs = config.epochs\n",
        "schedule_interval = 1\n",
        "\n",
        "# settings for the model\n",
        "fast_transformer = config.fast_transformer\n",
        "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
        "embsize = config.layer_size  # embedding dimension\n",
        "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
        "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
        "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
        "dropout = config.dropout  # dropout probability\n",
        "\n",
        "# logging\n",
        "log_interval = 100  # iterations\n",
        "save_eval_interval = config.save_eval_interval  # epochs\n",
        "do_eval_scib_metrics = True"
      ],
      "metadata": {
        "id": "PnPIWuF9g7Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate settings\n",
        "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
        "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
        "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
        "if input_style == \"binned\":\n",
        "    if input_emb_style == \"scaling\":\n",
        "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
        "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
        "    if input_emb_style == \"category\":\n",
        "        raise ValueError(\n",
        "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
        "        )\n",
        "\n",
        "if input_emb_style == \"category\":\n",
        "    mask_value = n_bins + 1\n",
        "    pad_value = n_bins  # for padding gene expr values\n",
        "    n_input_bins = n_bins + 2\n",
        "else:\n",
        "    mask_value = -1\n",
        "    pad_value = -2\n",
        "    n_input_bins = n_bins\n",
        "\n",
        "if ADV and DAB:\n",
        "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
        "DAB_separate_optim = True if DAB > 1 else False"
      ],
      "metadata": {
        "id": "4nYp4R6DnS1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = config.dataset_name\n",
        "save_dir = Path(\"/content/drive/MyDrive/projects/scGPT-MAFLD/finetune/\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"save to {save_dir}\")\n",
        "logger = scg.logger\n",
        "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pQ7b291nxDd",
        "outputId": "663a9ca4-f617-4b26-d27f-065bcb63b3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save to /content/drive/MyDrive/projects/scGPT-MAFLD/finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and pre-process data"
      ],
      "metadata": {
        "id": "xEWFkrQjo2XT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2THFSPIoqxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}